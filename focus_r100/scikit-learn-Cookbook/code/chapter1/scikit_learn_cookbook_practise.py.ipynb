{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "This notebook will cover the following topics:\n",
    ">--> Getting sample data from external sources\n",
    ">--> Creating sample data for toy analysis\n",
    ">--> Confirming the characteristics of created data\n",
    ">--> Scaling data to the standard normal\n",
    ">--> Creating binary features through thresholding\n",
    ">--> Working with categorical variables\n",
    ">--> Binarizing label features\n",
    ">--> Imputing missing values through various strategies\n",
    ">--> Using Pipelines for multiple preprocessing steps\n",
    ">--> Reducing dimensionality with PCA\n",
    ">--> Using factor analytics for decomposition\n",
    ">--> Kernel PCA for nonlinear dimensionality reduction\n",
    ">--> Using truncated SVD to reduce dimensionality\n",
    ">--> Decomposition to classify with DictionaryLearning\n",
    ">--> Putting it all together with Pipelines\n",
    ">--> Using Gaussian processes for regression\n",
    ">--> Defining the Gaussian process object directly\n",
    ">--> Using stochastic gradient descent for regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#library objjects and functions\n",
    "\n",
    "#ML specific\n",
    "from sklearn import datasets as d\n",
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.preprocessing import LabelBinarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading Cal. housing from http://www.dcc.fc.up.pt/~ltorgo/Regression/cal_housing.tgz to /home/manavee/scikit_learn_data\n"
     ]
    }
   ],
   "source": [
    "# Getting sample data from external sources\n",
    "\n",
    "#pre-installed with scikit library\n",
    "boston = d.load_boston() \n",
    "\n",
    "#download with scikit-learn\n",
    "housing = d.fetch_california_housing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([20, 80])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Creating sample data for toy analysis\n",
    "    # Generate a random regression problem.\n",
    "        # make_regression(n_samples=100, n_features=100, n_informative=10, n_targets=1, bias=0.0, effective_rank=None, tail_strength=0.5, noise=0.0, shuffle=True, coef=False, random_state=None)\n",
    "    # Generate a random n-class classification problem\n",
    "        # make_classification(n_samples=100, n_features=20, n_informative=2, n_redundant=2, n_repeated=0, n_classes=2, n_clusters_per_class=2, weights=None, flip_y=0.01, class_sep=1.0, hypercube=True, shift=0.0, scale=1.0, shuffle=True, random_state=None)\n",
    "    # Generate isotropic Gaussian blobs for clustering\n",
    "        #make_blobs(n_samples=100, n_features=2, centers=3, cluster_std=1.0, center_box=(-10.0, 10.0), shuffle=True, random_state=None)\n",
    "    # datasets.make_biclusters\n",
    "    # datasets.make_blobs\n",
    "    # datasets.make_checkerboard\n",
    "    # datasets.make_circles\n",
    "    # datasets.make_classification\n",
    "    # datasets.make_friedman1\n",
    "    # datasets.make_friedman2\n",
    "    # datasets.make_friedman3\n",
    "    # datasets.make_gaussian_quantiles\n",
    "    # datasets.make_hastie_10_2\n",
    "    # datasets.make_low_rank_matrix\n",
    "    # datasets.make_moons\n",
    "    # datasets.make_multilabel_classification\n",
    "    # datasets.make_regression\n",
    "    # datasets.make_s_curve\n",
    "    # datasets.make_sparse_coded_signal\n",
    "    # datasets.make_sparse_spd_matrix\n",
    "    # datasets.make_sparse_uncorrelated\n",
    "    # datasets.make_spd_matrix\n",
    "    # datasets.make_swiss_roll        \n",
    "\n",
    "complex_reg_data = d.make_regression(1000, 10, 5, 2, 1.0)\n",
    "classification_set = d.make_classification(weights=[0.2])\n",
    "Xs,Ys = d.make_blobs()\n",
    "\n",
    "#bincount(x, weights=None, minlength=None)\n",
    "    #Count number of occurrences of each value in array of non-negative ints\n",
    "np.bincount(classification_set[1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  3.59376071  11.36363636  11.13677866]\n",
      "[  8.58828355  23.29939569   6.85357058]\n",
      "[  6.34099712e-17  -6.34319123e-16  -2.68291099e-15]\n"
     ]
    }
   ],
   "source": [
    "#Scaling data to the standard normal\n",
    "    #MinMaxScaler()\n",
    "        #arr_scaled = arr_std * (arr_max - arr_min) + arr_min\n",
    "        #use feature_range=(min,max) for defining new min,max\n",
    "    #StandardScaler()\n",
    "         #Standardize features by removing the mean and scaling to unit variance\n",
    "         #x = ( x - mean(X) )/ std(X)\n",
    "\n",
    "X, y = boston.data, boston.target\n",
    "print(X[:, :3].mean(axis=0)) # mean of first 3 columns\n",
    "print(X[:, :3].std(axis=0)) #standard deviation first 3 columns\n",
    "scaler = preprocessing.StandardScaler()\n",
    "print(scaler.fit(X[:, :3]).transform(X[:, :3]).mean(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# >--> Creating binary features through thresholding\n",
    "#   What:\n",
    "#       -Instead of working with the distribution to standardize it, we'll purposely throw away data\n",
    "#   Why:\n",
    "#      -Often, in what is ostensibly continuous data, there are discontinuities that can be determined via binary features\n",
    "#   How:\n",
    "#         preprocessing.binarize #(a function)\n",
    "#         preprocessing.Binarizer #(a class)\n",
    "#   When:\n",
    "#         -Convert continous feature to categorical\n",
    "\n",
    "# If the value is greater than the threshold i.e mean, produce a 1; if it is less, produce a 0:\n",
    "new_target = preprocessing.binarize(boston.target, threshold=boston.target.mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# text_encoder = preprocessing.OneHotEncoder()\n",
    "#    definition:\n",
    "#         Encode categorical integer features using a one-hot aka one-of-K scheme\n",
    "#     input:\n",
    "#          The input to this transformer should be a matrix of integers, denoting values taken on by categorical (discrete) features\n",
    "#     output:\n",
    "#          a sparse matrix where each column corresponds to one possible value of one feature\n",
    "#     USecase:\n",
    "#         Given a dataset with three features and two samples, we let the encoder find the maximum value per feature and transform the data to a binary one-hot encoding\n",
    "\n",
    "#DictVectorizer()\n",
    "#     definition:\n",
    "#         -transforms lists of mappings (dict-like objects) of feature names to feature values into Numpy arrays or scipy.sparse matrices\n",
    "#             -strings -> one-hot (aka one-of-K) coding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.]])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# >--> Working with categorical variables\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "d = np.column_stack((X, y))\n",
    "text_encoder = preprocessing.OneHotEncoder()\n",
    "text_encoder.fit_transform(d[:, -1:]).toarray()[:5]\n",
    "dv = DictVectorizer()\n",
    "my_dict = [{'species': iris.target_names[i]} for i in y]\n",
    "dv.fit_transform(my_dict).toarray()[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.]])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#help(DictVectorizer())\n",
    "dv = DictVectorizer()\n",
    "my_dict = [{'species': iris.target_names[i]} for i in y]\n",
    "dv.fit_transform(my_dict).toarray()[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-73-4eee0e9bc076>, line 10)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-73-4eee0e9bc076>\"\u001b[0;36m, line \u001b[0;32m10\u001b[0m\n\u001b[0;31m    print(v.get_feature_names())\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "v = DictVectorizer()\n",
    "D = [{'foo': 1, 'bar': 2}, {'foo': 3, 'baz': 1}]\n",
    "X = v.fit_transform(D)\n",
    "support = SelectKBest(chi2, k=2).fit(X, [0, 1])\n",
    "print(v.get_feature_names())\n",
    "# ['bar', 'baz', 'foo']\n",
    "print(v.restrict(support.get_support())\n",
    "# doctest: +ELLIPSIS\n",
    "# DictVectorizer(dtype=..., separator='='sort=True,sparse=True)\n",
    "print(v.get_feature_names())\n",
    "# ['bar', 'foo']"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    ">--> Binarizing label features\n",
    "    -LabelBinarizer(neg_label=0, pos_label=1, sparse_output=False):\n",
    "        -Binarize labels in a one-vs-all fashion\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target old [0 0 0 0 0] \n",
      " new_target.shape   (150, 3) \n",
      " new_target[:5]   [[1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]] \n",
      " label_binarizer.classes_   [0 1 2] \n",
      " label_binarizer.transform([4])   [[0 0 0]] \n",
      " end\n",
      "[[ 1000 -1000 -1000]\n",
      " [ 1000 -1000 -1000]\n",
      " [ 1000 -1000 -1000]\n",
      " [ 1000 -1000 -1000]\n",
      " [ 1000 -1000 -1000]]\n"
     ]
    }
   ],
   "source": [
    "# d.load_iris()\n",
    "target = iris.target\n",
    "label_binarizer = LabelBinarizer()\n",
    "new_target = label_binarizer.fit_transform(target)\n",
    "print('target old', target[:5],\n",
    "      '\\n new_target.shape  ', new_target.shape,\n",
    "      '\\n new_target[:5]  ',new_target[:5],\n",
    "      '\\n label_binarizer.classes_  ',label_binarizer.classes_,\n",
    "      '\\n label_binarizer.transform([4])  ', label_binarizer.transform([4]),\n",
    "      '\\n end')\n",
    "\n",
    "label_binarizer = LabelBinarizer(neg_label=-1000, pos_label=1000)\n",
    "print(label_binarizer.fit_transform(target)[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    ">--> Imputing missing values through various strategies:\n",
    "    -Imputation transformer for completing missing values\n",
    "    -preprocessing.Imputer(missing_values='NaN', strategy='mean', axis=0, verbose=0, copy=True)\n",
    "        -strategy- \"mean\"/\"median\"/\"most_frequent\"\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ True False False False]\n",
      " [False  True False False]\n",
      " [False False  True False]\n",
      " [False  True  True  True]\n",
      " [False  True False False]]\n",
      "[[ nan -1.   1.4  0.2]\n",
      " [ 4.9  nan -1.   0.2]\n",
      " [-1.  -1.   nan -1. ]\n",
      " [-1.   nan  nan  nan]\n",
      " [-1.   nan  1.4 -1. ]]\n",
      "[[-1.  -1.   1.4  0.2]\n",
      " [ 4.9 -1.  -1.   0.2]\n",
      " [-1.  -1.  -1.  -1. ]\n",
      " [-1.  -1.  -1.  -1. ]\n",
      " [-1.  -1.   1.4 -1. ]]\n"
     ]
    }
   ],
   "source": [
    "iris_X = iris.data\n",
    "masking_array = np.random.binomial(1, .25, \n",
    "                    iris_X.shape).astype(bool)\n",
    "iris_X[masking_array] = np.nan\n",
    "print(masking_array[:5])\n",
    "print(iris_X[:5])\n",
    "impute = preprocessing.Imputer(strategy='median')\n",
    "iris_X_prime = impute.fit_transform(iris_X)\n",
    "print(iris_X_prime[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object `astype` not found.\n"
     ]
    }
   ],
   "source": [
    "astype?\n",
    "pd.DataFrame.fillna?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
